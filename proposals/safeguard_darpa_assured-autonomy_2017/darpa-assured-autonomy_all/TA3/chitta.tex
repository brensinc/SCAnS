\subsection{Learning of assurance rules}


%The learning enabled autonomous systems will contain learning enabled components. To have a common formal representation of various modules and components of the learning enabled autonomous systems, we aim to develop learning enabled components that can be characterized using the common representation. For a seamless integration of such components, we aim to develop tools and algorithms that can learn rules in ASP and L from data.

Although there are some existing algorithms for learning assurance rules in ASP \cite{ray2009nonmonotonic,athakravi2013learning,law2014inductive,athakravi2015inductive,iled,kazmi2017improving}, they do not work well with large data sets. The best performing among them is the system XHAIL and the general sub-area of learning logic rules from data is called inductive logic programming (ILP). 

In statistical machine learning, it is common to learn a function from a series of $\langle x, y \rangle$ pairs where $x$ denotes the input and $y$ denotes the desired output. On the other hand, ILP deals with learning a logic program $H$ given some background knowledge and a dataset containing positive and negative examples. More formally, Given a set of positive examples $E^{+}$, negative examples $E^{−}$ and some background knowledge $B$, an ILP algorithm \cite{muggleton1991inductive} finds a hypothesis $H$ such that, $B \cup H  \models E^{+}$, $B \cup H \not \models E^{−}$. The possible hypothesis space is often restricted with a language bias that is specified by a series of mode declarations $M$.

Because of the mismatch between statistical machine learning and ILP definitions, one needs to convert all the $\langle x,y \rangle$ to create a global $E^{+}$ and $E^{−}$ and extra care needs to be taken so that different $\langle x,y \rangle$   pairs do not interfere with each other.  This approach fails to scale up. That is because, when we have a large  number of $\langle x,y \rangle$  pairs, the ILP solvers will then have to deal with large ASP programs,  We propose to develop a novel iterative and incremental approach where instead of making the above conversion, the learning of rules is done by iteratively going over each example $\langle x,y \rangle$ pair.  Our approach is based on context dependent learning, defined as follows:

{\bf Definition}:	A \textit{Context Dependent Learning} task $ILP^{ctx}$ is a tuple $\langle B,M,D \rangle$, where $B$ is an Answer Set Program, called the background knowledge, $M$ defines the set of rules allowed in hypotheses (the hypothesis space) and $D$ is the dataset containing a series of context dependent examples $E_1,E_2,..., E_n$. Here each $E_i$ is a tuple $\langle O_i,E_{i}^{+}, E_{i}^{-} \rangle$ where,  $O_i$ is a logic program, called \textit{observation} , $E^{+}$ is a set of positive ground literals and $E^{-}$ is a set of negative ground literals. A hypothesis $H$ is an inductive solution of $T$ (written as $H \in ILP^{ctx}(B,M,D)$) \textit{iff}
	$H\cup B \cup O_i \vdash E_{i}^{+}, ~\forall i=1...n$,  and   
	$H\cup B \cup O_i \not \vdash E_{i}^{-}, ~\forall i=1...n$. 

In this formulation, each example $E_i$ directly corresponds to an $\langle x,y \rangle$ pair and it takes into account that there are several distinct examples in a dataset. 
We aim to develop an algorithm that will find the solutions incrementally. First it will compute the solutions of $ILP^{ctx}(\langle B,M,{E_1}\rangle)$, then it will use those to find the solutions $ILP^{ctx}(\langle B,M,{E_1, E_2} \rangle)$ and so on until it finds a solution for the original problem, $ILP^{ctx}(\langle B,M,{E_1,...E_n}\rangle)$. Thus, our algorithm will not compute the solutions of $ILP^{ctx}(\langle B,M,{E_1, E_2,..E_i}\rangle)$ in a single step. Rather it will compute several lower bound-upper bound pairs on the search space and employ a state space search for each lower bound-upper bound pair until all the minimal solutions in that space are found. 

Another aspect that we will address in learning of assurance rules is allowing previously undefined predicates in learning $H$. Currently, in all ILP systems and algorithms the mode definitions of rules in $H$ do not allow previously undefined predicates. However, such predicates are needed to learn rules whose conditional part may contain universal quantifications. The universal quantification plays an important role in the specification of temporal operators ``{\em always}'' and ``{\em until}''.

\subsection{Enhancing L with uncertainty}

We propose to enhance L with various uncertainty related features. Although there have been various earlier proposals that incorporate uncertainty to the underlying answer set program framework of L, including our own (P-log) we propose to develop a new formalism that incorporates the features crucial to assurance with respect to autonomous systems. Such features include causality, ability to do causal and counter-factual reasoning, use of weights (instead of direct probability numbers, which are hard to estimate for assurance rules) from which probabilities are computed, and probabilistic non-monotonicity — the ability to have new possible models as a result of new information. Probabilistic non-monotonicity is useful for non-naive conditioning, as naive conditioning may fail with new sensors that ask different kind of questions and as a result need a new set of possible worlds. (Such a situation is described in \cite{halpern2003} and addressed in the context of ASP by us in \cite{baral2007}.) 

To allow non-naive conditioning we need rules of the form:

random(o(Y) : \{X : p(X)\}) :- 
$a_1, \ldots , a_n, \ {\bf not} \ b_1, \ldots, \ {\bf not} \ b_m$. 

The above rules state that $o(Y)$ is a random variable where $Y$ can take the value from the set \{X : p(X)\} when $B$ is true. Such a construct allows non-monotonic specification of when $o(Y)$ is a random variable and what its extent is. 

Such non-monotonic specification of randomness is not possible in the popular uncertainty formalisms such as Markov Logic Networks or its recently proposed ASP variant $LP^{MLN}$. While P-log has such a construct, it lacks in its ability to specify arbitrary weights. 

%In recent years several extensions of ASP have been proposed to account for uncertainty. One approach, as used in P-log enhances ASP with some explicit probability assignments and additional implicitly derived probability numbers. In another approach, as used in $LP^{MLN}$ (and even in the much earlier introduced ASP with weak constraints), weight assignments are given explicitly and probability numbers are computed on top of them. We aim to explore several alternatives with an eye on efficient reasoning. 

%For efficient implementation of our proposed enhancement of L with uncertainty, one approach we plan to explore is modularizing the ASP programs and making restrictions on the modules. For example, an ASP program with two modules, with the bottom part allowing disjunctions and the top part restricted to be a stratified program. 

%Another approach we plan to explore is the use of soft logic, as done in probabilistic soft logic. 

\subsection{Solver Speed improvement of L}

We will improve the reasoning engine associated with L.  There exists several excellent inference engines for basic ASP. This includes the Clingo and Potassco suite \footnote{http://potassco.sourceforge.net/}, smodels, and dlv for basic ASP and some initial systems for languages such as P-log and $LP^{MLN}$ that augment ASP with probabilities and weights respectively. We will develop a more efficient system by modularizing the ASP programs and making restrictions on the modules. For example, we will consider ASP program with two modules, with the bottom part allowing disjunctions and the top part restricted to be a stratified program.  Such an ASP program is as expressive as ASP programs without any restrictions. However, reasoning over stratified programs can be done in quadratic time. Thus, instead of using general purpose ASP solvers, specialized efficient solvers can be built to reason with stratified ASP programs. This approach can then be extended to ASP programs with multiple modules where the dependency of the modules define a partial order.  



